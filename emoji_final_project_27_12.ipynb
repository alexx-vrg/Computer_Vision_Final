{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "FFJTTAohrhVv",
        "outputId": "38c983dc-8087-477e-eb70-cb8e69c45466"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4162928909.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 1. Mount Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# 2. Define the target directory for the extracted dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Mount Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Define the target directory for the extracted dataset\n",
        "DATASET_TARGET_DIR = \"/content/drive/MyDrive/FinalProject/\"\n",
        "TRAIN_PATH = os.path.join(DATASET_TARGET_DIR, \"train\")\n",
        "TEST_PATH = os.path.join(DATASET_TARGET_DIR, \"test\")\n",
        "LABELS_CSV = os.path.join(DATASET_TARGET_DIR, \"train_labels.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TIkCx03sZ-3",
        "outputId": "ada0db77-3ab5-4558-b9bb-7b50b0f252e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config: EfficientNetB0 | 224x224 | Batch 16\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pathlib\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "import os\n",
        "\n",
        "# --- ULTRA CONFIG ---\n",
        "IMG_HEIGHT = 224  # Standard EfficientNet\n",
        "IMG_WIDTH = 224\n",
        "BATCH_SIZE = 16   # Reduced to fit in GPU RAM\n",
        "SEED = 42\n",
        "\n",
        "# Reproducibility\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "print(f\"Config: EfficientNetB0 | {IMG_HEIGHT}x{IMG_WIDTH} | Batch {BATCH_SIZE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrcwlGEJscCH",
        "outputId": "5742ea74-5f8c-4e89-896c-645753eaf8fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline ready with Augmentation.\n"
          ]
        }
      ],
      "source": [
        "# 1. Load Labels\n",
        "labels_df = pd.read_csv(LABELS_CSV)\n",
        "class_names = sorted(labels_df[\"Label\"].unique().tolist())\n",
        "num_classes = len(class_names)\n",
        "label_to_index = {label: idx for idx, label in enumerate(class_names)}\n",
        "id_to_label = dict(zip(labels_df[\"Id\"].astype(str).str.zfill(5), labels_df[\"Label\"]))\n",
        "\n",
        "train_image_paths = sorted([str(p) for p in pathlib.Path(TRAIN_PATH).glob(\"*.png\")])\n",
        "train_labels = []\n",
        "valid_paths = []\n",
        "\n",
        "for path in train_image_paths:\n",
        "    img_id = os.path.splitext(os.path.basename(path))[0]\n",
        "    if img_id in id_to_label:\n",
        "        train_labels.append(label_to_index[id_to_label[img_id]])\n",
        "        valid_paths.append(path)\n",
        "\n",
        "# 2. Load Functions\n",
        "def load_img(path, label):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_png(img, channels=3)\n",
        "\n",
        "    # Feature Size (Original)\n",
        "    orig_shape = tf.cast(tf.shape(img)[:2], tf.float32)\n",
        "    size_feat = orig_shape / 256.0\n",
        "\n",
        "    # Resize High-Res (EfficientNet handles 0-255 internally usually, but explicit resize needed)\n",
        "    img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\n",
        "    return img, size_feat, label\n",
        "\n",
        "def augment(img, size, label):\n",
        "    # Data Augmentation (Train only)\n",
        "    img = tf.image.random_flip_left_right(img)\n",
        "    img = tf.image.random_brightness(img, max_delta=0.2)\n",
        "    img = tf.image.random_contrast(img, lower=0.8, upper=1.2)\n",
        "    img = tf.image.random_saturation(img, 0.8, 1.2)\n",
        "    # Random Zoom/Crop technique\n",
        "    zoom_size = int(IMG_HEIGHT * 1.1)\n",
        "    img = tf.image.resize(img, [zoom_size, zoom_size])\n",
        "    # Corrected random_crop usage\n",
        "    img = tf.image.random_crop(img, size=[IMG_HEIGHT, IMG_WIDTH, 3])\n",
        "    return {\"img\": img, \"size\": size}, label\n",
        "\n",
        "def no_augment(img, size, label):\n",
        "    return {\"img\": img, \"size\": size}, label\n",
        "\n",
        "# 3. Create Datasets\n",
        "ds_full = tf.data.Dataset.from_tensor_slices((valid_paths, train_labels))\n",
        "ds_full = ds_full.shuffle(len(valid_paths), seed=SEED)\n",
        "\n",
        "val_size = int(0.2 * len(valid_paths))\n",
        "train_ds = ds_full.skip(val_size).map(load_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_ds = train_ds.map(augment, num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_ds = ds_full.take(val_size).map(load_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.map(no_augment, num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(\"Pipeline ready with Augmentation.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "6T5G3us0sfMM",
        "outputId": "94ffb99d-0c86-458d-e7f0-f23f1886158d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ img (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│                     │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ efficientnetb0      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m,      │  \u001b[38;5;34m4,049,571\u001b[0m │ img[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │ \u001b[38;5;34m1280\u001b[0m)             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ efficientnetb0[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ size (\u001b[38;5;33mInputLayer\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1282\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ size[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │    \u001b[38;5;34m656,896\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)         │      \u001b[38;5;34m3,591\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ img (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ efficientnetb0      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │ img[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ efficientnetb0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ size (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1282</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ size[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">656,896</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,591</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,710,058\u001b[0m (17.97 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,710,058</span> (17.97 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m660,487\u001b[0m (2.52 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">660,487</span> (2.52 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,049,571\u001b[0m (15.45 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> (15.45 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def build_model():\n",
        "    img_input = layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3), name=\"img\")\n",
        "    size_input = layers.Input(shape=(2,), name=\"size\")\n",
        "\n",
        "    # EfficientNetB0 (Pre-trained)\n",
        "    # Note: EfficientNet includes normalization layers, expecting [0, 255] input.\n",
        "    base_model = EfficientNetB0(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n",
        "                                include_top=False,\n",
        "                                weights='imagenet')\n",
        "    base_model.trainable = False # Frozen start\n",
        "\n",
        "    x = base_model(img_input)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "\n",
        "    # Fusion\n",
        "    combined = layers.Concatenate()([x, size_input])\n",
        "    combined = layers.Dense(512, activation=\"relu\", kernel_regularizer=regularizers.l2(1e-4))(combined)\n",
        "    combined = layers.Dropout(0.5)(combined)\n",
        "\n",
        "    output = layers.Dense(num_classes, activation=\"softmax\")(combined)\n",
        "\n",
        "    model = models.Model(inputs=[img_input, size_input], outputs=output)\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(), # Label smoothing complex with sparse, keeping standard\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCcdf52lshC8",
        "outputId": "45207067-bc48-43cd-d1d5-e2376700ea5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase 1: Warmup (Head only)...\n",
            "Epoch 1/4\n",
            "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 664ms/step - accuracy: 0.5206 - loss: 1.3464 - val_accuracy: 0.8287 - val_loss: 0.6121\n",
            "Epoch 2/4\n",
            "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 94ms/step - accuracy: 0.7384 - loss: 0.8100 - val_accuracy: 0.8773 - val_loss: 0.4891\n",
            "Epoch 3/4\n",
            "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 91ms/step - accuracy: 0.7829 - loss: 0.6803 - val_accuracy: 0.9060 - val_loss: 0.4100\n",
            "Epoch 4/4\n",
            "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 89ms/step - accuracy: 0.8031 - loss: 0.6445 - val_accuracy: 0.9116 - val_loss: 0.3954\n"
          ]
        }
      ],
      "source": [
        "print(\"Phase 1: Warmup (Head only)...\")\n",
        "history_warm = model.fit(train_ds, validation_data=val_ds, epochs=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5CJiLUiGsiYj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a84d7f4-627f-451a-dc58-6aa200bb7052"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase 2: Fine-Tuning (Full Model)...\n",
            "Epoch 1/20\n",
            "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 161ms/step - accuracy: 0.8190 - loss: 0.6125 - val_accuracy: 0.9124 - val_loss: 0.3991 - learning_rate: 1.0000e-05\n",
            "Epoch 2/20\n",
            "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 90ms/step - accuracy: 0.8316 - loss: 0.5892 - val_accuracy: 0.9163 - val_loss: 0.3831 - learning_rate: 1.0000e-05\n",
            "Epoch 3/20\n",
            "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 91ms/step - accuracy: 0.8241 - loss: 0.6025 - val_accuracy: 0.9139 - val_loss: 0.3878 - learning_rate: 1.0000e-05\n",
            "Epoch 4/20\n",
            "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 90ms/step - accuracy: 0.8307 - loss: 0.6034 - val_accuracy: 0.9243 - val_loss: 0.3692 - learning_rate: 1.0000e-05\n",
            "Epoch 5/20\n",
            "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 89ms/step - accuracy: 0.8386 - loss: 0.5538 - val_accuracy: 0.9179 - val_loss: 0.3760 - learning_rate: 1.0000e-05\n",
            "Epoch 6/20\n",
            "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 90ms/step - accuracy: 0.8284 - loss: 0.6015 - val_accuracy: 0.9275 - val_loss: 0.3566 - learning_rate: 1.0000e-05\n",
            "Epoch 7/20\n",
            "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 90ms/step - accuracy: 0.8423 - loss: 0.5698 - val_accuracy: 0.9243 - val_loss: 0.3690 - learning_rate: 1.0000e-05\n",
            "Epoch 8/20\n",
            "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 89ms/step - accuracy: 0.8386 - loss: 0.5815 - val_accuracy: 0.9163 - val_loss: 0.3746 - learning_rate: 1.0000e-05\n",
            "Epoch 9/20\n",
            "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 90ms/step - accuracy: 0.8380 - loss: 0.5763 - val_accuracy: 0.9275 - val_loss: 0.3524 - learning_rate: 2.0000e-06\n",
            "Epoch 10/20\n",
            "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 89ms/step - accuracy: 0.8276 - loss: 0.5984 - val_accuracy: 0.9339 - val_loss: 0.3504 - learning_rate: 2.0000e-06\n",
            "Epoch 11/20\n",
            "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 93ms/step - accuracy: 0.8300 - loss: 0.5969 - val_accuracy: 0.9195 - val_loss: 0.3875 - learning_rate: 2.0000e-06\n",
            "Epoch 12/20\n",
            "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 90ms/step - accuracy: 0.8352 - loss: 0.5704 - val_accuracy: 0.9219 - val_loss: 0.3603 - learning_rate: 2.0000e-06\n",
            "Epoch 13/20\n",
            "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 89ms/step - accuracy: 0.8373 - loss: 0.5667 - val_accuracy: 0.9139 - val_loss: 0.3755 - learning_rate: 1.0000e-06\n",
            "Epoch 14/20\n",
            "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 91ms/step - accuracy: 0.8386 - loss: 0.5604 - val_accuracy: 0.9291 - val_loss: 0.3685 - learning_rate: 1.0000e-06\n",
            "Epoch 15/20\n",
            "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 91ms/step - accuracy: 0.8285 - loss: 0.6016 - val_accuracy: 0.9171 - val_loss: 0.3716 - learning_rate: 1.0000e-06\n"
          ]
        }
      ],
      "source": [
        "print(\"Phase 2: Fine-Tuning (Full Model)...\")\n",
        "\n",
        "model.trainable = True\n",
        "\n",
        "# Crucial Callbacks\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), # Very low LR\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "history_fine = model.fit(train_ds,\n",
        "                         validation_data=val_ds,\n",
        "                         epochs=20,\n",
        "                         callbacks=[early_stop, reduce_lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Jy6EN5vfskUk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "802c9e05-e842-4a52-c097-5020c6e0ed26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TTA Predictions (Test Time Augmentation)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission_ultimate.csv generated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f737c0aa-c2eb-4ead-b132-658acb423070\", \"submission_ultimate.csv\", 9)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "print(\"TTA Predictions (Test Time Augmentation)...\")\n",
        "\n",
        "test_paths = sorted([str(p) for p in pathlib.Path(TEST_PATH).glob(\"*.png\")])\n",
        "submission_ids = []\n",
        "predictions = []\n",
        "\n",
        "for path in tqdm.tqdm(test_paths):\n",
        "    # Load raw\n",
        "    img_raw = tf.io.read_file(path)\n",
        "    img = tf.image.decode_png(img_raw, channels=3)\n",
        "\n",
        "    # Feature\n",
        "    orig_shape = tf.cast(tf.shape(img)[:2], tf.float32)\n",
        "    size_feat = orig_shape / 256.0\n",
        "\n",
        "    # Resize\n",
        "    img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH]) # [0-255] float32\n",
        "\n",
        "    # Create Batch: [Original, Flipped]\n",
        "    img_flip = tf.image.flip_left_right(img)\n",
        "    batch_imgs = tf.stack([img, img_flip])\n",
        "    batch_sizes = tf.stack([size_feat, size_feat])\n",
        "\n",
        "    # Predict\n",
        "    preds = model.predict({\"img\": batch_imgs, \"size\": batch_sizes}, verbose=0)\n",
        "\n",
        "    # Average probabilities\n",
        "    avg_pred = np.mean(preds, axis=0)\n",
        "    predictions.append(class_names[np.argmax(avg_pred)])\n",
        "    submission_ids.append(os.path.splitext(os.path.basename(path))[0])\n",
        "\n",
        "# Save\n",
        "sub_df = pd.DataFrame({\"Id\": submission_ids, \"Label\": predictions})\n",
        "sub_df.to_csv(\"submission_ultimate.csv\", index=False)\n",
        "print(\"submission_ultimate.csv generated.\")\n",
        "files.download(\"submission_ultimate.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49e80c10"
      },
      "source": [
        "### Ensure Dataset is Extracted\n",
        "\n",
        "It appears the test dataset might not have been extracted yet, leading to an empty submission file. This step will check if the `train` and `test` directories exist, and if not, it will attempt to extract them from a `finalproject.zip` file located in your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d4e3d96",
        "outputId": "8ae47d64-56a8-4a1e-a635-3817edd6b507"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "# Define paths\n",
        "zip_path = \"/content/drive/MyDrive/finalproject.zip\" # Adjust if your zip file is named differently or in another location\n",
        "\n",
        "# Check if dataset directories exist\n",
        "if not os.path.exists(TRAIN_PATH) or not os.path.exists(TEST_PATH):\n",
        "    print(\"Dataset directories (train/test) not found. Attempting to extract from zip file...\")\n",
        "    if os.path.exists(zip_path):\n",
        "        try:\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(DATASET_TARGET_DIR)\n",
        "            print(f\"Dataset extracted to {DATASET_TARGET_DIR}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting zip file: {e}\")\n",
        "            print(\"Please ensure 'finalproject.zip' is correctly placed and not corrupted.\")\n",
        "    else:\n",
        "        print(f\"Error: Zip file not found at {zip_path}.\")\n",
        "        print(\"Please upload 'finalproject.zip' to your Google Drive or specify the correct path.\")\n",
        "else:\n",
        "    print(\"Dataset directories already exist. Skipping extraction.\")\n",
        "\n",
        "# Verify that images are now present in TEST_PATH\n",
        "if not os.listdir(TEST_PATH):\n",
        "    print(f\"Warning: {TEST_PATH} is still empty after extraction attempt. Please check the zip file content and path.\")\n",
        "else:\n",
        "    print(f\"Found {len(os.listdir(TEST_PATH))} files in {TEST_PATH}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset directories already exist. Skipping extraction.\n",
            "Warning: /content/drive/MyDrive/FinalProject/test is still empty after extraction attempt. Please check the zip file content and path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abe46d51"
      },
      "source": [
        "After running the above cell to ensure the dataset is present, you will need to re-run the cells that load the image paths and make predictions (specifically cells `KrcwlGEJscCH` and `Jy6EN5vfskUk`) for the changes to take effect."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}