{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe0b8c47",
   "metadata": {
    "papermill": {
     "duration": 0.004556,
     "end_time": "2025-12-01T11:50:49.031024",
     "exception": false,
     "start_time": "2025-12-01T11:50:49.026468",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Emoji classification\n",
    "## Imports\n",
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdae888",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-01T11:50:49.039302Z",
     "iopub.status.busy": "2025-12-01T11:50:49.038980Z",
     "iopub.status.idle": "2025-12-01T11:51:05.852568Z",
     "shell.execute_reply": "2025-12-01T11:51:05.851194Z"
    },
    "papermill": {
     "duration": 16.819946,
     "end_time": "2025-12-01T11:51:05.854417",
     "exception": false,
     "start_time": "2025-12-01T11:50:49.034471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "Sample training files:\n",
      "./train\\00001.png\n",
      "./train\\00002.png\n",
      "./train\\00003.png\n",
      "./train\\00004.png\n",
      "./train\\00005.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "# Deep Learning imports - using PyTorch for better compatibility\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Print sample files from local data\n",
    "print(\"\\nSample training files:\")\n",
    "train_path = \"./train\"\n",
    "if os.path.exists(train_path):\n",
    "    files = os.listdir(train_path)[:5]\n",
    "    for f in files:\n",
    "        print(os.path.join(train_path, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69dc83b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T11:51:05.863892Z",
     "iopub.status.busy": "2025-12-01T11:51:05.862748Z",
     "iopub.status.idle": "2025-12-01T11:51:35.124311Z",
     "shell.execute_reply": "2025-12-01T11:51:35.123246Z"
    },
    "papermill": {
     "duration": 29.268454,
     "end_time": "2025-12-01T11:51:35.126628",
     "exception": false,
     "start_time": "2025-12-01T11:51:05.858174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from skimage import io, color\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6947512d",
   "metadata": {
    "papermill": {
     "duration": 0.003767,
     "end_time": "2025-12-01T11:51:35.134476",
     "exception": false,
     "start_time": "2025-12-01T11:51:35.130709",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c3126c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T11:51:35.144206Z",
     "iopub.status.busy": "2025-12-01T11:51:35.143610Z",
     "iopub.status.idle": "2025-12-01T11:51:35.153447Z",
     "shell.execute_reply": "2025-12-01T11:51:35.152366Z"
    },
    "papermill": {
     "duration": 0.01691,
     "end_time": "2025-12-01T11:51:35.155544",
     "exception": false,
     "start_time": "2025-12-01T11:51:35.138634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_single_image(path, size=(64, 64)):\n",
    "    \"\"\"Load and preprocess a single emoji image.\"\"\"\n",
    "    img = Image.open(path).convert(\"RGBA\")\n",
    "    \n",
    "    # Create white background and composite image (handles transparency)\n",
    "    background = Image.new(\"RGB\", img.size, (255, 255, 255))\n",
    "    background.paste(img, mask=img.split()[3])  # Use alpha channel as mask\n",
    "    img = background\n",
    "    \n",
    "    # Resize to target size\n",
    "    img = img.resize(size, Image.LANCZOS)\n",
    "    \n",
    "    return np.array(img).astype(\"float32\") / 255.0\n",
    "\n",
    "\n",
    "class EmojiDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for emoji classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, file_paths, labels_dict=None, label_encoder=None, \n",
    "                 transform=None, img_size=(64, 64), is_test=False):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels_dict = labels_dict\n",
    "        self.label_encoder = label_encoder\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "        self.is_test = is_test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.file_paths[idx]\n",
    "        img_id = Path(img_path).stem\n",
    "        \n",
    "        # Load image\n",
    "        img = load_single_image(img_path, self.img_size)\n",
    "        \n",
    "        # Convert to tensor (H, W, C) -> (C, H, W)\n",
    "        img = torch.tensor(img).permute(2, 0, 1)\n",
    "        \n",
    "        # Apply transforms if any\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        # Return based on mode\n",
    "        if self.is_test:\n",
    "            return img, img_id\n",
    "        else:\n",
    "            # Get label\n",
    "            label_str = self.labels_dict.get(img_id, self.labels_dict.get(int(img_id)))\n",
    "            label = self.label_encoder.transform([label_str])[0]\n",
    "            return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d078b9",
   "metadata": {
    "papermill": {
     "duration": 0.004142,
     "end_time": "2025-12-01T11:51:35.164144",
     "exception": false,
     "start_time": "2025-12-01T11:51:35.160002",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a6fe9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T11:51:35.173666Z",
     "iopub.status.busy": "2025-12-01T11:51:35.172746Z",
     "iopub.status.idle": "2025-12-01T11:51:35.177789Z",
     "shell.execute_reply": "2025-12-01T11:51:35.176634Z"
    },
    "papermill": {
     "duration": 0.011591,
     "end_time": "2025-12-01T11:51:35.179536",
     "exception": false,
     "start_time": "2025-12-01T11:51:35.167945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use local path (data is in the same directory as this notebook)\n",
    "PATH = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7a5821",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T11:51:35.190065Z",
     "iopub.status.busy": "2025-12-01T11:51:35.188956Z",
     "iopub.status.idle": "2025-12-01T11:51:41.609047Z",
     "shell.execute_reply": "2025-12-01T11:51:41.607769Z"
    },
    "papermill": {
     "duration": 6.427634,
     "end_time": "2025-12-01T11:51:41.611072",
     "exception": false,
     "start_time": "2025-12-01T11:51:35.183438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dir = Path(PATH + \"train/\")\n",
    "train_files = sorted([str(p) for p in train_dir.iterdir() if p.is_file()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0172a2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T11:51:41.620701Z",
     "iopub.status.busy": "2025-12-01T11:51:41.620397Z",
     "iopub.status.idle": "2025-12-01T11:51:47.752941Z",
     "shell.execute_reply": "2025-12-01T11:51:47.751917Z"
    },
    "papermill": {
     "duration": 6.139414,
     "end_time": "2025-12-01T11:51:47.754931",
     "exception": false,
     "start_time": "2025-12-01T11:51:41.615517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dir = Path(PATH + \"test/\")\n",
    "test_files = sorted([str(p) for p in test_dir.iterdir() if p.is_file()])\n",
    "test_ids = [Path(f).stem for f in test_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675589f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T11:51:47.766730Z",
     "iopub.status.busy": "2025-12-01T11:51:47.766404Z",
     "iopub.status.idle": "2025-12-01T11:51:47.793766Z",
     "shell.execute_reply": "2025-12-01T11:51:47.792429Z"
    },
    "papermill": {
     "duration": 0.035073,
     "end_time": "2025-12-01T11:51:47.795604",
     "exception": false,
     "start_time": "2025-12-01T11:51:47.760531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train_df = pd.read_csv(PATH+ \"train_labels.csv\")\n",
    "\n",
    "y_train_dct = dict(zip(y_train_df[\"Id\"], y_train_df[\"Label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fda3bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T11:51:47.805270Z",
     "iopub.status.busy": "2025-12-01T11:51:47.804665Z",
     "iopub.status.idle": "2025-12-01T11:51:47.811792Z",
     "shell.execute_reply": "2025-12-01T11:51:47.810132Z"
    },
    "papermill": {
     "duration": 0.014286,
     "end_time": "2025-12-01T11:51:47.813901",
     "exception": false,
     "start_time": "2025-12-01T11:51:47.799615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 7\n",
      "Classes: ['apple' 'facebook' 'google' 'messenger' 'mozilla' 'samsung' 'whatsapp']\n",
      "Training samples: 8397\n",
      "Validation samples: 1482\n"
     ]
    }
   ],
   "source": [
    "# Create label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train_df[\"Label\"])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Classes: {label_encoder.classes_}\")\n",
    "\n",
    "# Split training data for validation\n",
    "train_files_split, val_files = train_test_split(\n",
    "    train_files, test_size=0.15, random_state=42\n",
    ")\n",
    "print(f\"Training samples: {len(train_files_split)}\")\n",
    "print(f\"Validation samples: {len(val_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23e7aa7",
   "metadata": {
    "papermill": {
     "duration": 0.003681,
     "end_time": "2025-12-01T11:51:47.821889",
     "exception": false,
     "start_time": "2025-12-01T11:51:47.818208",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c023555",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T11:51:47.834550Z",
     "iopub.status.busy": "2025-12-01T11:51:47.834131Z",
     "iopub.status.idle": "2025-12-01T11:51:47.842502Z",
     "shell.execute_reply": "2025-12-01T11:51:47.841381Z"
    },
    "papermill": {
     "duration": 0.01805,
     "end_time": "2025-12-01T11:51:47.844647",
     "exception": false,
     "start_time": "2025-12-01T11:51:47.826597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batches: 66\n",
      "Validation batches: 12\n",
      "Test batches: 78\n"
     ]
    }
   ],
   "source": [
    "# Simplified data augmentation (faster on CPU)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.3),\n",
    "])\n",
    "\n",
    "# OPTIMIZED: Smaller images = much faster training\n",
    "IMG_SIZE = (32, 32)  # Reduced from 64x64 (4x fewer pixels!)\n",
    "BATCH_SIZE = 128     # Larger batches for efficiency\n",
    "\n",
    "train_dataset = EmojiDataset(\n",
    "    train_files_split, y_train_dct, label_encoder,\n",
    "    transform=train_transform, img_size=IMG_SIZE, is_test=False\n",
    ")\n",
    "val_dataset = EmojiDataset(\n",
    "    val_files, y_train_dct, label_encoder,\n",
    "    transform=None, img_size=IMG_SIZE, is_test=False\n",
    ")\n",
    "test_dataset = EmojiDataset(\n",
    "    test_files, labels_dict=None, label_encoder=None,\n",
    "    transform=None, img_size=IMG_SIZE, is_test=True\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2a6d84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T11:51:47.854144Z",
     "iopub.status.busy": "2025-12-01T11:51:47.853758Z",
     "iopub.status.idle": "2025-12-01T11:51:47.864613Z",
     "shell.execute_reply": "2025-12-01T11:51:47.863411Z"
    },
    "papermill": {
     "duration": 0.018326,
     "end_time": "2025-12-01T11:51:47.867010",
     "exception": false,
     "start_time": "2025-12-01T11:51:47.848684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmojiCNN(\n",
      "  (block1): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (block2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (block3): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=1)\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): Linear(in_features=64, out_features=7, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Total parameters: 102,407\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LIGHTWEIGHT CNN MODEL - Optimized for fast CPU training\n",
    "# ============================================================================\n",
    "\n",
    "class EmojiCNN(nn.Module):\n",
    "    \"\"\"Lightweight CNN for fast emoji platform classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes):\n",
    "        super(EmojiCNN, self).__init__()\n",
    "        \n",
    "        # Block 1: 32x32x3 -> 16x16x32\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        \n",
    "        # Block 2: 16x16x32 -> 8x8x64\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        \n",
    "        # Block 3: 8x8x64 -> 4x4x128\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        \n",
    "        # Global Average Pooling + Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model = EmojiCNN(num_classes).to(device)\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fddc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.002, weight_decay=1e-4)  # Higher LR for faster convergence\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Training\", leave=False)\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.3f}', 'acc': f'{100.*correct/total:.1f}%'})\n",
    "    \n",
    "    return running_loss / len(loader), 100. * correct / total\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss / len(loader), 100. * correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2627e5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training (optimized for CPU)...\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 1/15] | Train Loss: 1.7811 | Train Acc: 25.51% | Val Loss: 1.9149 | Val Acc: 21.39% | LR: 0.002000 *** BEST ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 2/15] | Train Loss: 1.5559 | Train Acc: 37.18% | Val Loss: 1.5853 | Val Acc: 36.64% | LR: 0.002000 *** BEST ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 3/15] | Train Loss: 1.4284 | Train Acc: 43.27% | Val Loss: 1.5782 | Val Acc: 37.85% | LR: 0.002000 *** BEST ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 4/15] | Train Loss: 1.3166 | Train Acc: 48.87% | Val Loss: 1.4478 | Val Acc: 45.01% | LR: 0.002000 *** BEST ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 5/15] | Train Loss: 1.2287 | Train Acc: 53.40% | Val Loss: 1.6918 | Val Acc: 37.72% | LR: 0.002000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 6/15] | Train Loss: 1.1699 | Train Acc: 55.91% | Val Loss: 1.4481 | Val Acc: 48.04% | LR: 0.002000 *** BEST ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 7/15] | Train Loss: 1.0883 | Train Acc: 59.59% | Val Loss: 1.7008 | Val Acc: 35.83% | LR: 0.002000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 8/15] | Train Loss: 1.0489 | Train Acc: 61.33% | Val Loss: 2.1737 | Val Acc: 27.33% | LR: 0.002000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 9/15] | Train Loss: 1.0082 | Train Acc: 62.74% | Val Loss: 3.2323 | Val Acc: 28.68% | LR: 0.002000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/15] | Train Loss: 0.9452 | Train Acc: 65.33% | Val Loss: 2.0085 | Val Acc: 42.91% | LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/15] | Train Loss: 0.8745 | Train Acc: 68.08% | Val Loss: 1.3512 | Val Acc: 51.75% | LR: 0.001000 *** BEST ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/15] | Train Loss: 0.8324 | Train Acc: 70.14% | Val Loss: 1.4396 | Val Acc: 50.54% | LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/15] | Train Loss: 0.7978 | Train Acc: 71.59% | Val Loss: 1.4214 | Val Acc: 55.87% | LR: 0.001000 *** BEST ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/15] | Train Loss: 0.7859 | Train Acc: 71.63% | Val Loss: 1.2755 | Val Acc: 55.60% | LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/15] | Train Loss: 0.7769 | Train Acc: 72.69% | Val Loss: 1.2082 | Val Acc: 56.41% | LR: 0.001000 *** BEST ***\n",
      "======================================================================\n",
      "Training complete! Best Validation Accuracy: 56.41%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRAINING LOOP - OPTIMIZED FOR SPEED\n",
    "# ============================================================================\n",
    "\n",
    "NUM_EPOCHS = 15  # Reduced from 50 - model converges fast\n",
    "PATIENCE = 5     # Reduced from 10\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"Starting training (optimized for CPU)...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(val_acc)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_emoji_model.pth')\n",
    "        patience_counter = 0\n",
    "        marker = \" *** BEST ***\"\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        marker = \"\"\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1:2d}/{NUM_EPOCHS}] | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% | \"\n",
    "          f\"LR: {current_lr:.6f}{marker}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_emoji_model.pth', weights_only=True))\n",
    "print(\"=\" * 70)\n",
    "print(f\"Training complete! Best Validation Accuracy: {best_val_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb234ae1",
   "metadata": {},
   "source": [
    "## Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc70d840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions on test set...\n",
      "Total predictions: 9879\n",
      "Sample predictions: ['google', 'messenger', 'messenger', 'whatsapp', 'messenger', 'google', 'messenger', 'facebook', 'apple', 'mozilla']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MAKE PREDICTIONS ON TEST SET\n",
    "# ============================================================================\n",
    "\n",
    "def predict_test(model, test_loader, label_encoder):\n",
    "    \"\"\"Make predictions on test set.\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_ids = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, img_ids in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            # Convert predictions to labels\n",
    "            pred_labels = label_encoder.inverse_transform(predicted.cpu().numpy())\n",
    "            all_predictions.extend(pred_labels)\n",
    "            all_ids.extend(img_ids)\n",
    "    \n",
    "    return all_ids, all_predictions\n",
    "\n",
    "# Make predictions\n",
    "print(\"Making predictions on test set...\")\n",
    "test_ids_pred, y_test_pred = predict_test(model, test_loader, label_encoder)\n",
    "print(f\"Total predictions: {len(y_test_pred)}\")\n",
    "print(f\"Sample predictions: {y_test_pred[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31300ec5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T11:51:47.877760Z",
     "iopub.status.busy": "2025-12-01T11:51:47.876306Z",
     "iopub.status.idle": "2025-12-01T11:51:47.905692Z",
     "shell.execute_reply": "2025-12-01T11:51:47.904276Z"
    },
    "papermill": {
     "duration": 0.03674,
     "end_time": "2025-12-01T11:51:47.907893",
     "exception": false,
     "start_time": "2025-12-01T11:51:47.871153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission shape: (9879, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10002</td>\n",
       "      <td>messenger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10003</td>\n",
       "      <td>messenger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10004</td>\n",
       "      <td>whatsapp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10005</td>\n",
       "      <td>messenger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10006</td>\n",
       "      <td>google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10007</td>\n",
       "      <td>messenger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10008</td>\n",
       "      <td>facebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10009</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10010</td>\n",
       "      <td>mozilla</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id      Label\n",
       "0  10001     google\n",
       "1  10002  messenger\n",
       "2  10003  messenger\n",
       "3  10004   whatsapp\n",
       "4  10005  messenger\n",
       "5  10006     google\n",
       "6  10007  messenger\n",
       "7  10008   facebook\n",
       "8  10009      apple\n",
       "9  10010    mozilla"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create submission dataframe\n",
    "submission_df = pd.DataFrame({\n",
    "    \"Id\": test_ids_pred,\n",
    "    \"Label\": y_test_pred\n",
    "})\n",
    "\n",
    "print(f\"Submission shape: {submission_df.shape}\")\n",
    "submission_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf431098",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T11:51:47.918697Z",
     "iopub.status.busy": "2025-12-01T11:51:47.918362Z",
     "iopub.status.idle": "2025-12-01T11:51:47.939475Z",
     "shell.execute_reply": "2025-12-01T11:51:47.938082Z"
    },
    "papermill": {
     "duration": 0.02915,
     "end_time": "2025-12-01T11:51:47.941734",
     "exception": false,
     "start_time": "2025-12-01T11:51:47.912584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"final_submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14708949,
     "sourceId": 124850,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 67.667405,
   "end_time": "2025-12-01T11:51:50.835106",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-01T11:50:43.167701",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
